{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Librairies et GPU","metadata":{}},{"cell_type":"code","source":"!pip install GPUtil","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:01.982444Z","iopub.execute_input":"2023-04-05T13:20:01.983716Z","iopub.status.idle":"2023-04-05T13:20:15.344773Z","shell.execute_reply.started":"2023-04-05T13:20:01.983668Z","shell.execute_reply":"2023-04-05T13:20:15.343568Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7409 sha256=11fdeeea61c36815589ed642bddfd248ccec2c9934808454317beb7473db07b2\n  Stored in directory: /root/.cache/pip/wheels/b1/e7/99/2b32600270cf23194c9860f029d3d5db075f250bc39028c045\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"## Librairies\n\nfrom transformers import BertConfig,BertForMaskedLM, AutoModelForMaskedLM\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\nimport pandas as pd \nimport copy\nfrom datasets import load_metric\n\nimport random\nimport shutil\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\nimport time\nimport gc\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:15.348369Z","iopub.execute_input":"2023-04-05T13:20:15.349103Z","iopub.status.idle":"2023-04-05T13:20:27.423853Z","shell.execute_reply.started":"2023-04-05T13:20:15.349068Z","shell.execute_reply":"2023-04-05T13:20:27.422717Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"## GPU\ndevice='cuda' if torch.cuda.is_available() else 'cpu'\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:27.425277Z","iopub.execute_input":"2023-04-05T13:20:27.427030Z","iopub.status.idle":"2023-04-05T13:20:27.498791Z","shell.execute_reply.started":"2023-04-05T13:20:27.426989Z","shell.execute_reply":"2023-04-05T13:20:27.497660Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Fonctions Utiles","metadata":{}},{"cell_type":"code","source":"## Fonctions utiles\n\n# Masquage de 15% d'un texte\ndef Masks(Tokens):\n  L_indice=torch.zeros_like(Tokens).to(device)\n  tokenised_new=copy.deepcopy(Tokens) ## Masques \n\n  for i in range (len(Tokens)):\n      for proba in range (int(0.15 * float(Tokens[i].tolist().index(102)))):\n          mask_indice=random.randrange(start=1, stop=Tokens[i].tolist().index(102)-1)\n          tokenised_new[i][mask_indice]=103\n          L_indice[i][mask_indice]=1\n  return(tokenised_new,L_indice)\n\n# Création Matrice Pseudo-Attention (prend en compte les mots voisins à poids égals)\ndef matrice_pseudo_attention_cbow(weight,window_size,L_size):\n    window_size1=np.c_[np.zeros(L_size) ,weight*np.identity(L_size)[:,:-1] ] + weight*np.identity(L_size) + weight*np.c_[np.identity(L_size)[:,1:], np.zeros(L_size) ]\n    window_size2=window_size1+np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.identity(L_size)[:,:-2]]]\n    window_size3=window_size2 + np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.identity(L_size)[:,:-3]]]]\n    window_size4=window_size3 + np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.identity(L_size)[:,:-4]]]]]\n    window_size5=window_size4 + np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.c_[np.zeros(L_size),np.identity(L_size)[:,:-5]]]]]]\n \n    if window_size==2:\n        att=window_size2\n    if window_size==3:\n        att=window_size3\n    if window_size==4:\n        att=window_size4\n    if window_size==5:\n        att=window_size5\n    else:\n        att=window_size1  # par défaut aussi\n    return(torch.from_numpy(att))\n\n# Calcul de l'accuracy\ndef accuracy_score(Liste_de_logits,verite_terrain):\n    score=0\n    for i in range (len(verite_terrain)): #pour chaque mot masqué\n        y_top=(-Liste_de_logits[i]).argsort()[:TOP]  #les mots les plus probables selon Y\n        if verite_terrain[i] in y_top:\n            score+=1\n    return(score/len(verite_terrain))\n\n#Passer des probas en pourcentage\n\ndef MultiplyPourcent(Liste):\n    for i in range (len(Liste)):\n        Liste[i]=Liste[i]*100\n    return(Liste)\n\ndef DividePourcent(Liste):\n    for i in range (len(Liste)):\n        Liste[i]=Liste[i]/100\n    return(Liste)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:27.501731Z","iopub.execute_input":"2023-04-05T13:20:27.502114Z","iopub.status.idle":"2023-04-05T13:20:27.521145Z","shell.execute_reply.started":"2023-04-05T13:20:27.502075Z","shell.execute_reply":"2023-04-05T13:20:27.520227Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Paramètres Généraux","metadata":{}},{"cell_type":"code","source":"## Paramètres\n\nMAX_SEQ_LEN = 512\nLENGTH_BATCH=10000\nBATCH_SIZE=10\nTOP=5\nepochs=20\nwindow_size=5 #Fenêtre du contexte: 1= 1 avant 1 après,2= 1 avant 2 après,  3= #2 avant 2 après,  4= #2 avant 3 après, 5= #3 avant 3 après\n\ncheckpoint=\"bert-base-uncased\"\nmodel = AutoModelForMaskedLM.from_pretrained(checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:27.522770Z","iopub.execute_input":"2023-04-05T13:20:27.523170Z","iopub.status.idle":"2023-04-05T13:20:34.949266Z","shell.execute_reply.started":"2023-04-05T13:20:27.523108Z","shell.execute_reply":"2023-04-05T13:20:34.948230Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1685a3f06eaf4b83b2ab5c4aee1a7e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fdaccc87b1b4233a294e5e109219321"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a249d1f31eb428f880f92b2664c1219"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3a17c568b0e41049a7ae2f6f4d4dba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb5b750e3b1e48a4b12937185c8af051"}},"metadata":{}}]},{"cell_type":"markdown","source":"## Chargement et Traitement du Dataset","metadata":{}},{"cell_type":"code","source":"## Chargement et traitement du dataset \n\nwith open(\"/kaggle/input/wikipedia-corpus/train (1).txt\") as my_file:\n    Input=my_file.read()\n\nListe_index=[]\nInput=Input.split('\\n')\nfor i in range (len(Input)):\n    if len(Input[i])>500:\n        Liste_index.append(i)\n\nInput_processed=[]\nfor i in range (0,len(Liste_index)-3,3):\n    string= \" \".join([Input[Liste_index[i]], Input[Liste_index[i+1]]])\n    string= \" \".join([string, Input[Liste_index[i+2]]])\n    Input_processed.append(string)\n    \nInput_sentences=Input_processed[:int(0.7*len(Input_processed))]\nTest_sentences=Input_processed[int(0.7*len(Input_processed)):]\n\nprint(len(Input_sentences))\nprint(len(Test_sentences))\n\nInput_sentences[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:34.950735Z","iopub.execute_input":"2023-04-05T13:20:34.951131Z","iopub.status.idle":"2023-04-05T13:20:35.388069Z","shell.execute_reply.started":"2023-04-05T13:20:34.951092Z","shell.execute_reply":"2023-04-05T13:20:35.386987Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2433\n1044\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"\"Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism. Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more. Anarchism employs a diversity of tactics in order to meet its ideal ends which can be broadly separated into revolutionary and evolutionary tactics; there is significant overlap between the two, which are merely descriptive. Revolutionary tactics aim to bring down authority and state, having taken a violent turn in the past, while evolutionary tactics aim to prefigure what an anarchist society would be like. Anarchist thought, criticism, and praxis have played a part in diverse areas of human society. Criticism of anarchism include claims that it is internally inconsistent, violent, or utopian.\""},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokeniser et Masquages des mots","metadata":{}},{"cell_type":"code","source":"## Tokeniser\n\nInput_tokenised=tokenizer.batch_encode_plus(Input_sentences,return_tensors=\"pt\",padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True).to(device)\nTest_tokenised=tokenizer.batch_encode_plus(Test_sentences,return_tensors=\"pt\",padding=\"max_length\", max_length=MAX_SEQ_LEN, truncation=True).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:35.389381Z","iopub.execute_input":"2023-04-05T13:20:35.390022Z","iopub.status.idle":"2023-04-05T13:20:44.923751Z","shell.execute_reply.started":"2023-04-05T13:20:35.389983Z","shell.execute_reply":"2023-04-05T13:20:44.922700Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"## Masking\n\n# Input_tokenised_masked,L_indice=Masks(Input_tokenised)\n# Test_tokenised_masked,L_indice2=Masks(Test_tokenised)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:44.925034Z","iopub.execute_input":"2023-04-05T13:20:44.925379Z","iopub.status.idle":"2023-04-05T13:20:44.930128Z","shell.execute_reply.started":"2023-04-05T13:20:44.925342Z","shell.execute_reply":"2023-04-05T13:20:44.929083Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Notre modèle","metadata":{}},{"cell_type":"code","source":"## Modèle\n\nclass MLM(nn.Module):\n    def __init__(self,bert):\n        super(MLM, self).__init__()\n\n        self.embedding= nn.Embedding(30522,768)\n\n        self.v = nn.Linear(768, 30522,bias = False)\n        \n        self.v.weight = self.embedding.weight\n\n#         self.v.weight = torch.nn.Parameter(model.bert.embeddings.word_embeddings.weight,requires_grad=True)\n\n    def forward(self, tokens):\n        emb_init = self.embedding(tokens)\n        A=matrice_pseudo_attention_cbow(1,window_size,emb_init.shape[1]).to(device)\n        emb_init_context=torch.matmul(A.float(),emb_init)\n        return self.v(emb_init_context)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:44.931679Z","iopub.execute_input":"2023-04-05T13:20:44.932325Z","iopub.status.idle":"2023-04-05T13:20:44.941772Z","shell.execute_reply.started":"2023-04-05T13:20:44.932288Z","shell.execute_reply":"2023-04-05T13:20:44.940798Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Création des batchs","metadata":{}},{"cell_type":"code","source":"## Batchs des données \n\nclass TextDataset(Dataset):\n\n    def __init__(self, X):\n        self.inputs = X\n#         self.masques = M\n#         self.verite_terrain=torch_verite_terrain\n\n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, index):\n#         return (self.inputs[index],self.masques[index],self.verite_terrain[index])\n        return (self.inputs[index])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:44.945845Z","iopub.execute_input":"2023-04-05T13:20:44.946179Z","iopub.status.idle":"2023-04-05T13:20:44.952674Z","shell.execute_reply.started":"2023-04-05T13:20:44.946153Z","shell.execute_reply":"2023-04-05T13:20:44.951557Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Création des batchs\n\ndata_train= TextDataset(Input_tokenised[\"input_ids\"])\n\n# data_test= TextDataset(\n#     Test_tokenised_masked[\"input_ids\"],\n#     L_indice2,\n#     Test_tokenised[\"input_ids\"])\n\ntrain_loader = DataLoader(data_train,batch_size=BATCH_SIZE,shuffle=False)\n    \n# test_loader = DataLoader(data_test,batch_size=BATCH_SIZE,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:44.954009Z","iopub.execute_input":"2023-04-05T13:20:44.954463Z","iopub.status.idle":"2023-04-05T13:20:44.966270Z","shell.execute_reply.started":"2023-04-05T13:20:44.954426Z","shell.execute_reply":"2023-04-05T13:20:44.965350Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"# ## Training de notre modèle\n\n#Notre modèle\n# cbow=MLM(model)\n# model.to(device)\n# cbow.to(device)\n\n# Loss \n# CEL = nn.CrossEntropyLoss()\n# MSE= nn.MSELoss()\n\n#Optimizer\n# lr=0.001\n# optimizer = torch.optim.Adam(cbow.parameters(), lr )\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n\n# losses=[]\n# total_accuracy_train=[]\n# total_accuracy_test=[]\n\n# start = time.time()\n# for epoch in range(epochs):\n#     cbow.zero_grad()\n#     total_loss = 0\n    \n#     ## TRAIN\n#     for batch in tqdm((train_loader)):\n\n#         ## BATCH \n#         X,M,verite_terrain = batch\n        \n#         ## BERT ENTRAINE\n#         with torch.no_grad():\n#             Y_bert=model(X).logits   \n        \n#         ## MODEL\n#         Y_model=cbow(X)                           \n    \n#         ## LOSS\n#         loss_real=CEL(Y_model[M==1],F.one_hot(verite_terrain[M==1],num_classes=30522).to(torch.float32))\n#         loss_bert=MSE(Y_model[M==1],Y_bert[M==1])\n#         loss=loss_real + loss_bert\n#         total_loss+= loss_real.item() + loss_bert.item() \n\n#         ## ACCURACY \n#         accuracy_train=accuracy_score(Y_model[M==1],verite_terrain[M==1])\n\n#         loss.backward()\n#         optimizer.step()\n\n#     ## TEST\n#     for batch_test in (test_loader):\n#         X_test,M_test,verite_terrain_test = batch_test\n        \n#         ## MODEL\n#         Y_model_test=cbow(X_test)\n\n#         # ACCURACY \n#         accuracy_test=accuracy_score(Y_model_test[M_test==1],verite_terrain_test[M_test==1])\n\n#     print(\"Precision train: \" + str(round(accuracy_train*100.,2)) + \"% \")\n#     print(\"Precision test: \" + str(round(accuracy_test*100.,2)) + \"% \") \n \n#     losses.append(total_loss)\n#     total_accuracy_train.append(accuracy_train)\n#     total_accuracy_test.append(accuracy_test)\n\n# end = time.time()\n# time = end - start","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-04-05T13:20:44.967400Z","iopub.execute_input":"2023-04-05T13:20:44.967671Z","iopub.status.idle":"2023-04-05T13:20:44.976974Z","shell.execute_reply.started":"2023-04-05T13:20:44.967642Z","shell.execute_reply":"2023-04-05T13:20:44.975974Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"## Training Bert pas trained\n\n#Bert pas entrainé (pour comparaison)\nmy_config = BertConfig()\nmodel2 = BertForMaskedLM(my_config)\nmodel.to(device)\nmodel2.to(device)\n\n# Loss\nCEL=nn.CrossEntropyLoss()\nMSE= nn.MSELoss()\nlosses=[]\n\n#Optimizer\nlr2=1e-4\noptimizer = torch.optim.Adam(model2.parameters(), lr2 , betas=(0.9, 0.999))\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n\n# Steps\nNUM_ACCUMULATION_STEPS= 16\n\n# Accuracy\ntotal_accuracy_train=[]\ntotal_accuracy_test=[]\n\nfor epoch in range(epochs):\n    model2.zero_grad()\n    total_loss = 0\n    \n    ## TRAIN\n    for idx,X in enumerate (tqdm(train_loader)):\n\n        ## BATCH \n        Y,M = Masks(X)\n        \n        ## BERT PAS ENTRAINE\n        Logits_bert_not_trained=model2(Y).logits\n                \n        ## BERT ENTRAINE\n        with torch.no_grad():\n            Logits_bert=model(X).logits\n\n        ## LOSS\n        loss_real=CEL(Logits_bert_not_trained[M==1],F.one_hot(X[M==1],num_classes=30522).to(torch.float32))\n        loss_bert=MSE(Logits_bert_not_trained,Logits_bert)\n        loss= loss_bert + loss_real \n        loss = loss / NUM_ACCUMULATION_STEPS\n        \n        total_loss =  loss_bert.item() + loss_real.item()\n\n        loss.backward()\n        \n        if ((idx + 1) % NUM_ACCUMULATION_STEPS == 0) or (idx + 1 == len(train_loader)):\n            optimizer.step()\n\n        ## ACCURACY \n        accuracy_train=accuracy_score(Logits_bert_not_trained[M==1],X[M==1])\n#         accuracy_train_bert=accuracy_score(Y_bert[M==1],X[M==1])\n        total_accuracy_train.append(accuracy_train)\n        \n    gc.collect()\n    torch.cuda.empty_cache()\n    print(\"Precision train: \" + str(round(sum(total_accuracy_train)/len(total_accuracy_train)*100.,2)) + \"% \")\n\n    losses.append(total_loss)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-05T13:20:44.978371Z","iopub.execute_input":"2023-04-05T13:20:44.978912Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"100%|██████████| 244/244 [04:06<00:00,  1.01s/it]\n","output_type":"stream"},{"name":"stdout","text":"Precision train: 15.94% \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 244/244 [04:04<00:00,  1.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Precision train: 16.28% \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 244/244 [04:04<00:00,  1.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Precision train: 16.8% \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 244/244 [04:04<00:00,  1.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Precision train: 17.22% \n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 244/244 [04:04<00:00,  1.00s/it]\n","output_type":"stream"},{"name":"stdout","text":"Precision train: 17.51% \n","output_type":"stream"},{"name":"stderr","text":" 58%|█████▊    | 141/244 [02:21<01:45,  1.02s/it]","output_type":"stream"}]},{"cell_type":"code","source":"## Plot des Courbes de Trainings\n\nplt.figure()\nplt.subplot(2,1,1)\nplt.plot(MultiplyPourcent(total_accuracy_train),'r', label='accuracy de train')\nplt.plot(MultiplyPourcent(total_accuracy_test),'g', label='accuracy de test')\nplt.legend()\nplt.title('Accuracy train')\nplt.xlabel(\"Nombres d'époques\")\nplt.ylabel(\"Accuracy (en %)\")\nplt.subplot(2,1,2)\nplt.plot(losses)\nplt.title('Loss fonction')\nplt.xlabel(\"Nombres d'époques\")\nplt.ylabel(\"Mean square error\")\nplt.savefig(\"Loss.png\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Sauvegardes des paramètres \n\ntorch.save(cbow, 'best-model.pt') \nnbr_parameters= sum([np.prod(p.size()) for p in cbow.parameters()])\nprint(nbr_parameters)\nprint(time)\nparameters=np.array([[nbr_parameters],\n                     [cbow],\n                     [epochs],\n                     [time],\n                     [lr]])\n\nlignes_param = ['Nombres de Parametres: ', 'Model','Epoques: ' ,'Training time','Learning rate: ']\n\nparam_df = pd.DataFrame(data = parameters,index = lignes_param)\n\nparam_df.to_csv('Paramètres' +str(Nom)  + '.csv',sep =';')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from transformers import BertConfig,BertForMaskedLM\n# my_config = BertConfig()\n# model2 = BertForMaskedLM(my_config)\n# model2.bert.embeddings.word_embeddings.weight[10,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.bert.embeddings.word_embeddings.weight[10,1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}